{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f866af57",
   "metadata": {},
   "source": [
    "<html>\n",
    "<table width=\"100%\" cellspacing=\"2\" cellpadding=\"2\" border=\"1\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td valign=\"center\" align=\"center\" width=\"45%\"><img src=\"../media/Univ-Utah.jpeg\"><br>\n",
    "</td>\n",
    "    <td valign=\"center\" align=\"center\" width=\"75%\">\n",
    "<h1 align=\"center\"><font size=\"+1\">University of Utah<br>Population Health Sciences<br>Data Science Workshop</font></h1></td>\n",
    "<td valign=\"center\" align=\"center\" width=\"45%\"><img\n",
    "src=\"../media/U_Health_stacked_png_red.png\" alt=\"Utah Health\n",
    "Logo\" width=\"128\" height=\"134\"><br>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<br>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02047c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import medspacy\n",
    "import pandas as pd\n",
    "\n",
    "from medspacy.visualization import visualize_dep, visualize_ent, MedspaCyVisualizerWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3011b",
   "metadata": {},
   "source": [
    "# NLP Exercises\n",
    "The last few notebooks stepped through the major parts of a clinical NLP system. In this notebook you'll build a complete system for identifying [pneumonia](https://en.wikipedia.org/wiki/Pneumonia) in radiology reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5fa1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c5c0804",
   "metadata": {},
   "source": [
    "## 0. Load the data\n",
    "The dataset in these examples is a set of MIMIC-II radiology reports. The annotations were created by University of Utah physician-scientist and pneumonia extraordinaire [Dr. Barbara Jones](https://healthcare.utah.edu/fad/mddetail.php?physicianID=u0102859&name=barbara-e-jones). As baseline to compare our system against we will use a system recently developed by her team for identifying misdiagnosis of pneumonia in clinical notes: [`medspacy_pna`](https://github.com/abchapman93/medspacy_pneumonia). This was system was designed for VA and University of Utah data, so it might not achieve as high of performance on MIMIC data as what is reported in the paper. Let's see if we can beat its performance!\n",
    "\n",
    "The data is split into two sets: the **training set** and **testing set**. We'll start by developing our system with the training set before doing a final evaluation on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfcf6d",
   "metadata": {},
   "source": [
    "### 0.3 Read in the data\n",
    "Run the code below to read in the training set. The resulting dataframe will have a column for:\n",
    "- The document name\n",
    "- The text\n",
    "- The annotator's document classification (this is the **\"truth\"**)\n",
    "- The baseline NLP system's document classification (this is the **\"prediction\"**)\n",
    "\n",
    "We'll eventually add another column with our own predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_pneumonia_data(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad64ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984a250",
   "metadata": {},
   "source": [
    "## 1. Document annotation\n",
    "Before building an NLP system we need to define our concepts annotate a corpus of notes to use as a reference standard. We already have an annotated corpus, so we'll review a few short examples and see how we would annotate them and then look at the reference standard annotations that we already have.\n",
    "\n",
    "### 1.1\n",
    "For this task, we will define a **\"POS\"** note as: \n",
    "\n",
    "*A note which contains a positive **or** possible mention of a term referring to pneumonia.*\n",
    "\n",
    "Consider the following terms to be pneumonia:\n",
    "\n",
    "- Pneumonia\n",
    "- Pna\n",
    "- Opacity\n",
    "- Infiltrate\n",
    "- Consolidation\n",
    "\n",
    "Review the following notes and annotate each as 1 for positive or 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_pna_annotation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_pna_annotation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f9559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_pna_annotation3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_pna_annotation4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf99b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "142ba622",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "The true annotations can be found in `df[\"document_classification\"]`. Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_num_pna_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6672f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aeeb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_num_pos_pna_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0087ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69dcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f86f068a",
   "metadata": {},
   "source": [
    "## 2. Build your NLP system and process texts\n",
    "Now that we have some idea about what our dataset contains, let's starting building an NLP system and reviewing the output. First, build an empty NLP system. Then we'll process the notes in our dataset using our system as is (which doesn't have any rules). Go through the output and review the data. Find some examples of pneumonia that you should extract. Then go through and add rules for each of the following components as needed:\n",
    "\n",
    "1. Add target concept rules to `target_matcher` to identify pneumonia in the text\n",
    "2. Add ConText rules to `context` to improve attribute assertion\n",
    "3. Optionally, add additional rules to `sectionizer` if the section logic is helpful for classifying the entities.\n",
    "4. Build a document classifier which returns `0` or `1` for a doc. A simple version would just use the ConText attributes like `is_negated`, but a more complex version might also use information such as the section of the note.\n",
    "5. Evaluate the system and review errors\n",
    "\n",
    "After adding rules, reprocess your notes and review the output again. Since NLP is a computationally expensive procedure, you might want to work in batches of 10 or so before processing the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83327c",
   "metadata": {},
   "source": [
    "### 2.1: Load a model\n",
    "Import `medspacy` and create an `nlp` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO TEST VALUE\n",
    "test_load_nlp.test(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a1710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff722ec0",
   "metadata": {},
   "source": [
    "### 2.2: Add a Sectionizer\n",
    "By default, medspaCy doesn't load a `Sectionizer` component, but we want to include section detection in our pipeline. Add a sectionizer to your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921955c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"medspacy_sectionizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load_nlp_add_sectionizer.test(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474f520",
   "metadata": {},
   "source": [
    "### 2.3: Process notes\n",
    "The code below will process the notes in `df_train` with your NLP. If you want to use the whole dataset, set `df_train = df`. Otherwise, work in batches like `df_train = df.iloc[:10]`, `df_train = df.iloc[10:20]`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df\n",
    "df_train = df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e49024",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(df_train[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26fb98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = MedspaCyVisualizerWidget(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedf5ac",
   "metadata": {},
   "source": [
    "### 2.1 Concept extraction\n",
    "Add rules to the `target_matcher` component to extract mentions of pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbce128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.target_matcher import TargetRule\n",
    "target_rules = [\n",
    "\n",
    "]\n",
    "if len(target_rules) > 0:\n",
    "    nlp.get_pipe(\"medspacy_target_matcher\").add(target_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f89678",
   "metadata": {},
   "source": [
    "### 2.2 ConText\n",
    "Add any modifiers which were not captured with the default rule set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82abb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.context import ConTextRule\n",
    "context_rules = [\n",
    "\n",
    "]\n",
    "if len(context_rules) > 0:\n",
    "    nlp.get_pipe(\"medspacy_context\").add(context_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef057c",
   "metadata": {},
   "source": [
    "### 2.3 Sections\n",
    "Add any section titles which were not detected and led to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.section_detection import Sectionizer\n",
    "section_rules = [\n",
    "\n",
    "]\n",
    "\n",
    "if len(section_rules) > 0:\n",
    "    nlp.get_pipe(\"medspacy_sectionizer\").add(section_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d37adc",
   "metadata": {},
   "source": [
    "### 2.4: Document Classification\n",
    "Write a function called `classify_pna` which takes a doc and returns a `1` if the document is positive for pneumonia and `0` if it is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pna(doc):\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb4f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e10a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = classify_pna(nlp(\"There is no evidence of pna\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO TEST VALUE\n",
    "test_classify_pna_1.test(pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49718cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2 = classify_pna(nlp(\"Impression: pneumonia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf71e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO TEST VALUE\n",
    "test_classify_pna_2.test(pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aea17d",
   "metadata": {},
   "source": [
    "### 2.5: Evaluate your system on training data\n",
    "After reprocessing your texts and creating `docs` with an updated NLP, run the code below to get performance metrics for your system. The function `evaluate_system` will return a DataFrame with performance characteristics for your system as well as the baseline system.\n",
    "\n",
    "Look at the results and ask the following questions:\n",
    "- What sorts of mistakes does my system appear to be making?\n",
    "- Is precision or recall higher? What does that mean in the context of the research question?\n",
    "- How is it comparing to the baseline NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06643ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your predictions\n",
    "df_train = add_document_classifications(df_train, docs, classify_pna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = evaluate_system(df_train)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae623a4",
   "metadata": {},
   "source": [
    "### 2.6: Error Analysis\n",
    "Review examples of mistakes your NLP system made. We'll subset the dataframe to look at **false positives** and **false negatives**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a7824",
   "metadata": {},
   "source": [
    "First, edit the code below so that we have two different DataFrames containing errors: `fps` for false positibes and `fns` for false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6682743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = df_train.query(\"document_classification == 0 & nlp_document_classification == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = df_train.query(\"document_classification == 1 & nlp_document_classification == 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fps = MedspaCyVisualizerWidget(list(fps[\"doc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fns = MedspaCyVisualizerWidget(list(fns[\"doc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c068fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d35d956c",
   "metadata": {},
   "source": [
    "## 4. Final evaluation\n",
    "Once you feel like you're ready, read in the testing data, run your NLP on it, and evaluate it. You should do this **one time** so that it is an honest evaluation of how your system will perform on new, unseen data. Once you see the final results, go through the steps we did above with the training data to understand our performance on the testing set and what sorts of errors happened. How did your final system do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222cbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = read_pneumonia_data(\"test\")\n",
    "docs_test = list(nlp.pipe(df_test[\"text\"]))\n",
    "df_test = add_document_classifications(df_test, docs_test, classify_pna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_test[\"document_classification\"] == df_test[\"baseline_document_classification\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_system(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de6307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
